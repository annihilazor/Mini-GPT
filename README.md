# Mini-GPT
A GPT-style Transformer language model built entirely from scratch using Python and PyTorch â€” including a custom BPE tokenizer, multi-head causal self-attention, positional embeddings, and a full pre-training loop inspired by GPT-2 (124M).
